#SDN&NFV
>本文包括：

>1、SDN

>2、SDN 解决方案介绍

>3、NFV

>4、NFV 生态系统

>5、NFV 关键能力
##1、SDN
###引言
1. 计算机从大型机到我们的家用电脑有共同的架构：

	- 专业硬件包含 CPU 主板，内存，硬盘等

	- 在硬件之上有操作系统，再有应用软件

	- 另一方面用 PC 生态系统作比较，支撑 PC 生态系统快速革新的三个因素是 Hardware、Substrate、硬件底层化

	> PC 工业已经找到了一个简单通用的硬件底层: x86 指令集、Software-definition

3. ICT 发展启示
	- 大型机、个人电脑、手机都采用三层架构 ---- **硬件、操作系统、应用软件。**

	- NICK认为网络生态效仿PC生态，采用三层架构，就可以蓬勃发展

	- 支撑 SDN 的关键是找到一个合适的 Hardware Substrate, 就有了 OpenFlow。

4. OpenFlow
	- OpenFlow描述了对网络设备的一种抽象，其基本编程载体是 flow, 定义 flow、操作 flow、缓存 flow 等，这个协议是网络世界的 flow 指令集。

	- flow 指令集可以作为硬件架构和软件定义的一个桥梁，协议本身可以不断演进，下层的硬件架构可以跟着持续演进，上层的网络软件可以保持兼容

6. IP 网络面临的问题
	- 唯一路径  网络利用率低，IP网络既聪明又笨重
	- 协议复杂  维护故障定位困难
	- 缺少全局视图  不能全局最优

###SDN 介绍
5. SDN 核心思想
	- 控制和转发分离，软件应用灵活、可编程 ---- 这是源于 PC 和手机领域的变革

- SDN 的价值
	- 技术驱动   网络架构的变革
	- 网络   构架构建一个集中的大脑，实现全局流量和整体最优
	-关键价值   简化运维、自动化调度、提高网络利用率、网络开放

7. SDN 定义

	- Software define Network 即软件定义网络，由斯坦福大学 clean slate 研究组提出的，是一种新型网络创新架构。

	- 核心技术：通过将**网络设备控制面与数据面分离开来**，从而实现了**网络流量的灵活控制**，为核心网络及应用的创新提供了良好的平台。

	- 在网络中的表现：控制和转发是分开的，而且转发设备不再是专用设备。

8. SDN 网络与传统 IP 网络的区别？

	- SDN 利用控制转发相互分离从架构上解决根本问题：让网络敏捷起来，更快的部署新业务与快速定位故障点。采用资源集中和统一调度、能力开放的策略；让软件来干硬件的活；

- SDN 的三个架构层
	- SDN 应用
	- SDN 控制器
	- 物理网络


- NFV（Network Function Virtualization)：网络功能虚拟化，采用虚拟化技术，将传统电信设备的软件与硬件解耦，基于通用计算、存储、网络设备实现电信网络功能，提升管理和维护效率，增强系统灵活性

- SDN 与 NFV 的本质区别与联系
	- SDN 的关键特征
		- 集中控制、优化全局效率
		- 开放接口、加快业务上线
		- 网络抽象、屏蔽底层差异

	- NFV 关键特征
		- 上层业务云化，底层硬件标准化
		- 分层运营加快业务上线与创新

##2、SDN 解决方案介绍
###DCI
- 什么是 DCI？

	- DCI：Data center interconnect ， 指的是用于数据中心之间互联的网络，实现以数据中心为中心组网的基础承载网。

	- 未来超过 80% 的业务将部署在云上 我们云数据中心需要基于用户体验进行层次化布局，而网络则需要以数据中心为中心组网进行重构，在这样的大背景下，DCI 网络孕育而生。

- 为什么需要新建 DCI 网络？

	- 云数据中心的要求：高扩展性、低成本、资源丰富、温度适宜等条件，使得云数据中心建设位置要求 

		- 比如：某运营商新建大型云数据中心与传统骨干网位置不重合

	- 云业务对网络要求
		
		- 云计算对时延有非常严格的要求，如跨 DC 同步计算、虚拟机热迁移等业务要求都在 10ms 以下
		
		- DC 间流量具有突发性和不均衡性，需采用 SDN 计数进行实时智能调控，而现有网络复杂。新技术难部署。------ 很难重用现有骨干网，需要新建 DCI 网络。
			 
- 基于 SDN 的 DCI 方案总览

	- 顶层端到端协同，实现包含 DC 云与 DC 承载网的云网资源的一站式提供和端到端业务自动化协同发放。总的来说在多地区 多运营商部署多个数据中心的方式 目前已经成为了互联网行业普遍认可的最有效的解决用户覆盖提高用户业务体验的方案，建设并运营一张安全可靠、可灵活调度的多数据中心互联网络（DCI 网络），也成为了各大互联网公司在基础架构方面的工作之重，DCI 建成后 可以为宽带 4/3G 用户提供更好的访问体验外另一方面可以为互联网公司政府企业客户的云提供给更好的承载服务。
	- 现在 DCI 面临的实际问题：网络不灵活难以跟住业务快速迭代的步伐、链路利用率较低  以及居高不下的 OPEX 压力等。
	- 华为 SDN DCI 整体解决方案可以支撑云数据中心业务的端到端的运营，整体架构包括**承载层和控制层**，需要在网络基础承载层上引入部署 SDN 的控制层，控制层是网络的业务发放管理平台和网络智能控制中心，该层主要功能部件为：

		1. 业务发放平台：提供业务自动化入口实现租户业务自助发放以及网络资源状态的可视个运维管理入口
		- 业务协同平台：DCI 业务需求分解和 DC 和 IDC 的协同实现端到端的跨控制器资源的协同和分解
		- 云平台：接受业务发放平台的业务分解，进行 DC 运业务分解和协同，实现 DC 的内存储、算和网络的协同
		- DC 控制器：接受 OpenStack 业务分解同一控制 DC 的 NVE 和 VxLAN GW 实现 DC 内网络自动部署和控制
		- DCI 控制器：接受业务协同平台资源的分解，实现 Underlay 网络部署的自动化和网络流量的智能优化
		- 流量采集工具、调优策略的输入、流量采集组件可以基于端口 TE 隧道进行流量采集和分析并提供网络流量可视化界面

- DCI 骨干网解决方案承载层是租户业务的承载实体，负责跨 DC 网络的连接以及业务宽带和 SLA 保证

- 骨干网支持 VxLAN 技术提供了大二层组网的能力，能够跨越广域网和多个物理 DC 构建 vDC 网络, 实现跨区域的资源节点的互备和虚机动态迁移，有效提升了 DC 云资源的利用效率

- 骨干网部署业界广泛使用的 MPLS TE 流量工程技术为租户业务提供端到端的宽带保证，提升了网络资源的利用效率，特别是提供了基于租户和业务的差异化的服务能力

- 网络承载支持采用 Overlay 技术，Overlay 业务网络基于云业务驱动支持快速的业务开通

-  Underlay 物理网络按需提供网络资源，实现端到端的 SLA 保障和智能流量的优化

-  目前 IP Core 网络中存在如下一些流量调整需求：
	-  实现 IGW 出口、DC 出口的流量均衡 
	-  降低不同 ISP 网间费用的结算，将流量分配到费用较低的链路上 
	-  提升 VIP 用户体验  
	-  针对这些需求，当前主要依赖于手工调整 BGP 路由策略 ：
		1. 监控链路带宽利用率
		2. 识别出需要调整的流
        3. 基于流制作 BGP 策略下发给设别
        4. 循环操作，直到流量符合期望目标的要求

- 智能流量调优方案：RR + 方案

	- 手工方法不能实时调整，耗时长、配置和维护复杂问题，RR + 方案用于解决这问题。
	- RR + 方案在 IP core 现网中加入 SDN Controller，实现集中控制，智能化调优

- RR + 可以带来什么？
	1. 最大化 IGW 带宽利用率均衡链路间流量的分布，降低网间结算费用，不同客户提供不同 SLA 服务
	2. 自动调整流量，取代复杂的手工操作
	3. 基于标准 BGP 通讯，可以和现网设备平滑兼容。

###PCE+方案
- 什么是 PCE + 方案
	- 路由转发用最短路径算法不考虑带宽，存在利用率低的问题， PCE + 正是为了解决这一问题而诞生的
	- PCE + 通过在网络中不部署 PCE server（就是 SDN Controller），使用 StatefulPCE 技术，为 MPLS TE LSP 集中算路。使网络带宽资源使用尽量达到最优。
	- 该架构方案中需要新部署的网元是 PCE Server，转发设备为 PCE Client。
	- PCE Client 需要计算 LSP 时会向 PCE Server 发出路径计算请求，server 计算后结果反馈给 client，client 然后进行 LSP 隧道建立。

- 思考：什么是 DCI?
	DCI 即 Data center interconnect 指的是用于数据中心之间互联的网络 DCI 网络正是实现 “以数据为中心的中心组网” 的基础承载网。



##test
	) SDN 云数据中心场景使用 VXLAN 作为转发隧道，对于 BUM 报文设备会向所有的 VXLAN 隧道泛洪。B
	
	   A  对
	   B  错
	
	 VXLAN 集中式网关适合大型的数据中心。B
	
	   A  对
	   B  错
	
	 VXLAN 隧道不支持跨数据中心建立。B
	
	   A  对
	   B  错
	
	在 SDN 云网一体化场景中，AC 控制器通过什么协议和 OpenStack 的 Neutron 实现对接？B
	
	   A  Netconf
	   B  Restful
	   C  SNMP
	   D  OpenFlow
	
	 在 SDN 云数据中心场景中，AC 控制器通过什么协议向 underlay 网络中的设备下发配置？A
	
	   A  Netconf
	   B  OpenFlow
	   C  Restful
	   D  SNMP

##3、NFV
- CT 当前面临的结构性挑战
	- 增收方面：用户饱和，传统业务下滑
	- 节流方面：CT 投入成本下降，IT 部分的投入从 2002 年 6% 增加到 2013 年 13%，
	- 创新方面：CT 界 5 个/年， IT 界 160000 个/年 ，是32000 多倍
	- 商用速度：CT 每个月 6 个上市， IT 每小时 12 个

- 什么是 NFV？
	- NFV (Network Function Virtualization) 网络功能虚拟化，ETSI 组织下组建的。
	- NFV 是 IT 与 CT 结合的产物
	- 希望通过采用通用服务器、交换机和存储设备实现传统电信网络的功能。
	- 通过 IT 的虚拟化技术，许多类型的网络设备可以合并到工业界标准中，如 servers 、switchs 和 storages 。
	- 需要用软件实现网络功能并能在一系列工业标准服务器硬件上运行，可以根据需要迁移，实例化部署在网络的不同位置而不需要部署新设备
	- 关键诉求：需要大容量 Server 存储，大容量以太网，不同应用以软件形式远程自动部署在统一的基础设施上。
	- 三个关键点：软硬件解耦  开放  自动化
 
- NFV 将 IP 基因融入电信网络
	- 传统电信网软硬件绑定，更新困难，管理维护困难。
	- 采用虚拟化技术和云计算的网络，硬件采用标准的服务器、存储设备和交换机
	- 虚拟化之后 ，上层业务通过软件形式运行在统一的标准的硬件基础之上 。
	- 虚拟化后的网络好处：易于更新、硬件通用化 支持异构，资源归一  简化管理与运维
 
- NFV 正走向成熟
	- 2012年开始上升
	- 2013到2014年下降
	- 2015~2016 年稳步爬升 趋于成熟

##4、NFV 生态系统
1. NNFV 生态系统：
	- ETSI 在 2012 年成立了 NFV ISG 来研究网络功能虚拟化
	- 随后，涌现了一批 NFV 的开源组织，比如 OPNFV，OpenStack
	- NFV 产业联盟，秉承开发、创新、协同、落地的宗旨，集多长家和合作伙伴进行联合创新，成为开放联盟的引领者。

2. NFV 框架
	- NFV 框架主要包括 3 大组件：NFVI、VNF、和 MANO
解释：
	- 框架中最底层的是硬件，包括计算、存储、和网络资源
	- 往上是云操作系统，完成虚拟化和云化的相关的功能，硬件和云操作系统成为 NFVI。I 指的是 instruction，设施的意思，这些设都是有 VIM 来管理。
	- 再往上是虚拟网网络功能，比如 vIMS 提供 IMS 的语音业务，vEPC 提供 4G 的数据网络功能。虚拟网络功能由 VNFM 来管理，提供 VNF 的生命周期管理。
	- 再往上是网络管理层及网管，网管可以配套 NFVO 进行网络业务生命周期的管理

3. NFV 三大组件的关键要求
	- 组件 MANO：包括 NFVO、VNFM 和 VIM，
		- 要求 VNFM 适配不同厂商 NFVO 和 VIM；并且 MANO 系统应该尽量减少对现有的 OSS/BSS 的冲击。比如要求 MANO 支持和现有传统平台（如 U2000）的对接
	- 组件 VNF(虚拟化网络功能)：
		- 要求它可以运行在不同厂商的 NFVI；
		- 对应传统的电信业务网络，每个物理网元映射为一个虚拟网元 VNF。
	- 组件 NFVI - 云操作系统
		- 要求优选基于 OpenStack 的云操作系统
		- 将物理计算 / 存储 / 交换网络资源通过虚拟化计算转换为虚拟的计算 / 存储 / 交换资源池
	- 组件 NFVI - 硬件
		- 要求它优选具有虚拟化辅助功能的芯片的 COTS
		- 同时具备高 IOPS 与高可靠性的磁阵
		- 低 RAID 等级的磁阵建议冗余组网

##5、NFV 关键能力
1. 开放 ---- 广泛兼容，性能稳定，支持异构；
	- 开放的能力是指虚拟化网络功能运行在多厂商云平台；
	- NFV 支持异构：在硬件基础上，可以支持厂商 B 的 VNFM 和厂商 A 的 NFVO，并且可以在和现网的传统平台如 U2000 OSS 进行异构系统的集成；
	- NFV 可以广泛兼容不同厂商的硬件以及云化的操作系统

2. 云化架构（虚拟化！= 云化）
	- 云化架构是弹性和可靠性的基础。
	- 传统平台软件和硬件是绑定的；
	- 虚拟化阶段：软件和硬件进行了解耦，软件可以运行在标准的硬件基础上，但是业务逻辑和业务数据还是绑定的
	- 云化架构阶段: 软件和硬件继续解耦，同时业务逻辑和业务数据进行解耦，会话转发层和业务逻辑进行解耦。
	- 业界主要厂商当前能力和华为当前能力区别：程序与数据分离，转发与数据分离；支持水平扩容，内存分布式数据。

3.  弹性
	- 分钟级的弹性扩容和秒级弹性缩容。
	- 当业务量需要增加的时候，由主 RDB 生成新的虚拟机来支持更多的业务处理，RDB 中就保留了动态数据（用户签约数据、链路局向配置数据、稳态呼叫会话数据），因此用动态数据可以生成新的 VM 来支持业务需要；
	- 当业务量下降时，将业务迁移到其他虚拟机，对相应的虚拟机设备下电，减少虚拟机设备的运行，而稳态话务可以立即在其他模块中重建。

4. 高可靠性
	- 应用层、云操作系统层、硬件层都有相应的冗余机制。
	- 应用层高可靠性可以通过主备和负荷分担方式实现主备 VM 之间的冗余。主备冗余与无状态N+M。确保应用层会话 0 中断，99.999% 的可用性。
	- 云操作系统的可靠性可以通过虚拟机快速重建冗余机制来实现。
	- 硬件层高可靠性主要通过族化以及物料冗余机制来实现计算、存储、网络等硬件设备的冗余
	- 硬件层、VM 层、业务层各层可靠性各自独立，高度互补确保整体可用性。

5. 高性能
	- NFV 业界最权威的评估公司 SPECvirt。华为的 FusionSphere 性能得分为 4.6，排第一。
	- 呼叫处理方面华为的 FusionSphere 比第二名的 Vmware 高 17%。
	- 高性能技术的关键技术：NUMA 亲和性、CPU 绑定、DPDK、透明巨页、虚拟中断优化等

6. NFV 存在的问题

	- （1）标准不成熟，技术架构实现上有分歧；
	- （2）多供应商、集成复杂。
	- （3）部件兼容性风险大,NFV 只定义架构，都是开源组织自行定制。
	- （4）NFV 工程难度大，多厂家多方面。
	- （5）网络功能虚拟化技术滞后
	- （6）虚拟化可靠性不足。传统电信要求 99.999% 可靠性，所以需要完善

##test
	NFV 即 Network Functions Virtualization（网络功能虚拟化），就是将传统的 CT 业务部署到云平台上（云平台是指将物理硬件虚拟化所形成的虚拟机平台，能够承载 CT 和 IT 应用），从而实现软硬件解耦合。A
	
	   A  对
	   B  错
	
	VNF Descriptor (VNFD): 属于 TOSCA 标准的 YMAL 格式配置模板，主要描述了实例化一个 VNF 所需要的资源信息以及操作行为，主要用于 VNF 实例化以及生命周期管理。A
	
	   A  对
	   B  错
	
	 在 NFV 架构下，KVM 是基于 windows 内核特性，可借助于硬件辅助虚拟化技术，结合 Qemu 完成对 I/O 的模拟和操作，共同支撑多个 Guest OS 在同一套硬件上运行的虚拟化系统。B
	
	   A  对
	   B  错
	
	以下哪一项描述了 NFV 的本质？D
	
	   A  性能更好，速度更快
	   B  网络架构扁平化
	   C  万物互联
	   D  网络设备的 IT 化和云化	
	
	在 NFV 架构下，以下哪些是 IMS 切新平台及虚拟化的价值？BCD
	
	   A  增加 IMS 系统新特性
	   B  推动 TTM 大幅下降
	   C  降低 CAPEX/OPEX
	   D  提升网络灵活性和开放性